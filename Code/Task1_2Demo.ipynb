{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task1_2Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS7q695mQeO2"
      },
      "source": [
        "# Team Exsilio\n",
        "\n",
        "\n",
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSQ1P84-SVUd"
      },
      "source": [
        "import linecache\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "class CorpusReader:\n",
        "  \"\"\"This class indicates the Reader task for Task 1\"\"\"\n",
        "\n",
        "    # Creates the Dictionaries from the Path of the File\n",
        "\n",
        "  def create_dicts(self, path):\n",
        "    \"\"\"\n",
        "      Returns 2 Dictionaries. Once for the Line and One For the Relation in the File. \n",
        "      This is based on how the Data is organized in the supplied Data Files.\n",
        "    \"\"\"\n",
        "    line_d = {}\n",
        "    rel_d = {}\n",
        "\n",
        "    with open(path) as f:\n",
        "      for line in islice(f, 0, None, 4):\n",
        "        lister = line.split('\"')\n",
        "        line_number = int(lister[0].split('\\t')[0])\n",
        "        line_d[line_number] = ''.join(str(s) for s in lister[1:])\n",
        "    \n",
        "    with open(path) as f:\n",
        "      for i, line in enumerate(islice(f, 1, None, 4)):\n",
        "        rel_d[i] = line.split('\\n')[0]\n",
        "    \n",
        "    return (line_d, rel_d)\n",
        "\n",
        "  def create_dataframe(self, dictionary_to_convert, cols):\n",
        "\n",
        "    \"\"\"\n",
        "      From a Dictionary which is passed, and the desired column to create, this function\n",
        "      returns a Dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    dataframe_converted = pd.DataFrame.from_dict(dictionary_to_convert, orient='index', columns = cols)\n",
        "    dataframe_converted = dataframe_converted.reset_index()\n",
        "    dataframe_converted = dataframe_converted.drop(columns=['index'])\n",
        "\n",
        "    return dataframe_converted\n",
        "\n",
        "\n",
        "  def parse_data(self, path_to_file):\n",
        "\n",
        "    \"\"\"\n",
        "      Invokes the Create Dict and Create Data Frame Function.\n",
        "      This function is designed to create the Line and Relation Dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    line_dict, rel_dict = self.create_dicts(path_to_file)\n",
        "    \n",
        "    line_df = self.create_dataframe(line_dict, ['line'])\n",
        "    rel_df = self.create_dataframe(rel_dict, ['relation'])\n",
        "\n",
        "    line_df['relation'] = rel_df['relation']\n",
        "\n",
        "    return (line_df, rel_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6bk7oTjaT8W",
        "outputId": "d44b1d3d-9bec-49ae-d54f-0492dd096d5c"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "lines = CorpusReader()\n",
        "ans_df, rel_df = lines.parse_data('/content/test_sentence.txt')\n",
        "\n",
        "print(\"The Output of Task 1 is: \\n\")\n",
        "print(\"The Corpus has \", len(ans_df), \" sentences\\n\") \n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"The Parsed Line is             : \", ans_df['line'][index])\n",
        "  print(\"The Parsed Line has Relation   : \", ans_df['relation'][index])\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Output of Task 1 is: \n",
            "\n",
            "The Corpus has  2  sentences\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Parsed Line is             :  After handsome renovations at various locales, the <e1>company</e1> has remodeled a church into a <e2>home</e2>.\n",
            "\n",
            "The Parsed Line has Relation   :  Other\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Parsed Line is             :  The <e1>laptop</e1> was inside a protective <e2>plastic bag</e2> made from LDPE number 4, which can be easily recycled.\n",
            "\n",
            "The Parsed Line has Relation   :  Content-Container(e1,e2)\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAZN_xX4nfG5"
      },
      "source": [
        "# Task 2: All the required Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q4jTlKVejEW",
        "outputId": "f9e5d73b-0a29-4649-cea9-6e9f901e1411"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from nltk.corpus import wordnet as wn\n",
        "import re\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "class AllTasks(CorpusReader):\n",
        "\n",
        "  # Adding a column of tokens to the dataframe\n",
        "    def create_tokens(self, dataframe):\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "        For A DataFrame with the column 'line', this function will create tokens\n",
        "        of the words in that line\n",
        "\n",
        "        These tokens will be added as a New Column Named 'Tokens' in the DataFrame and will be returned\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      tokenize_dict = {}\n",
        "      filtered_token_dict = {}\n",
        "      iterator = dataframe.to_dict('dict')['line']\n",
        "      stopWords = ['e1', '/e1', 'e2', '/e2', '<', '>', '<e1>', '</e1>', '<e2>', '</e2>']\n",
        "      for key, val in iterator.items():\n",
        "        tokenize_dict[key] = nltk.word_tokenize(val)\n",
        "\n",
        "      for key, val in tokenize_dict.items():\n",
        "        all_tokens = []\n",
        "        filtered_tokens = []\n",
        "        for i in range(len(val)):\n",
        "          if val[i] == '<':\n",
        "            val[i] = ''.join(val[i:i+3])\n",
        "        \n",
        "          all_tokens = [e for e in val if e not in ('e1', 'e2', '/e1', '/e2', '>')]\n",
        "          filtered_tokens = [word for word in val if word not in stopWords]\n",
        "          filtered_token_dict[key] = ', '.join(str(word) for word in filtered_tokens)\n",
        "          tokenize_dict[key] = ', '.join(str(s) for s in all_tokens)\n",
        "\n",
        "      tokenize_dataframe = self.create_dataframe(tokenize_dict, ['token'])\n",
        "      filtered_tok_dataframe = self.create_dataframe(filtered_token_dict, ['filtered tokens'])\n",
        "      \n",
        "      dataframe['tokens'] = tokenize_dataframe['token']\n",
        "      dataframe['filtered tokens'] = filtered_tok_dataframe['filtered tokens']\n",
        "\n",
        "      return dataframe\n",
        "\n",
        "\n",
        "    def synctactic_parse(self, dataframe, col):\n",
        "      \n",
        "      \"\"\"\n",
        "\n",
        "        For A DataFrame with the window created, this function will add the Full Synctactic Parse.\n",
        "\n",
        "        These values will be added as Two Columns Named 'syn_parse' in the DataFrame and will be returned.\n",
        "\n",
        "      \"\"\"\n",
        "      \n",
        "      for i, val in enumerate(dataframe[col]):\n",
        "        print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "        print(\"Sentence : \", i+1, \"\\n\")\n",
        "\n",
        "        print(\"\\nText\\t\\t\\t\\t : Root Text\\t\\t\\t : Root Dep\\t\\t\\t : Root Head\\t\\t\\t : Root Head Pos\\t|\\n\")\n",
        "        print(\"------------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "        \n",
        "        s = sp(''.join(val).replace(',', ''))\n",
        "\n",
        "        for dep in s.noun_chunks:\n",
        "          print(dep.text, \"\\t\\t :\", dep.root.text, \"\\t\\t\\t :\", dep.root.dep_, \"\\t\\t\\t :\", dep.root.head.text, \"\\t\\t\\t :\", dep.root.head.pos_,\"\\t |\\n\")\n",
        "        print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "    def create_pos_dep_lemma(self, dataframe, col):\n",
        "      \"\"\"\n",
        "\n",
        "        For A DataFrame with the window created, this function will add the POS and Dep Tags of those words.\n",
        "\n",
        "        These values will be added as Two Columns Named 'pos' and 'dep' in the DataFrame and will be returned.\n",
        "\n",
        "      \"\"\"\n",
        "      pos_dict = {}\n",
        "      dep_dict = {}\n",
        "      lem_dict = {}\n",
        "      p = []\n",
        "      d = []\n",
        "      l = []\n",
        "      for i, val in enumerate(dataframe[col]):\n",
        "        s = sp(''.join(val).replace(',', ''))\n",
        "        for word in s:\n",
        "          p.append(word.pos_)\n",
        "          d.append(word.dep_)\n",
        "          l.append(word.lemma_)\n",
        "        pos_dict[i] = ', '.join(str(s) for s in p)\n",
        "        dep_dict[i] = ', '.join(str(s) for s in d)\n",
        "        lem_dict[i] = ', '.join(str(s) for s in l)\n",
        "        p = []\n",
        "        d = []\n",
        "        l = []\n",
        "      \n",
        "\n",
        "      colname1 = col + '_pos' if col in ['e1', 'e2', 'line window'] else 'pos'\n",
        "      colname2 = col + '_dep' if col in ['e1', 'e2', 'line window'] else 'dep'\n",
        "      colname3 = col + '_lem' if col in ['e1', 'e2', 'line window'] else 'lem'\n",
        "      pos_dataframe = self.create_dataframe(pos_dict, [colname1])\n",
        "      dep_dataframe = self.create_dataframe(dep_dict, [colname2])\n",
        "      lem_dataframe = self.create_dataframe(lem_dict, [colname3])\n",
        "\n",
        "      dataframe[colname1] = pos_dataframe[colname1]\n",
        "      dataframe[colname2] = dep_dataframe[colname2]\n",
        "      dataframe[colname3] = lem_dataframe[colname3]\n",
        "      return dataframe\n",
        "\n",
        "\n",
        "    def create_NER(self, dataframe):\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "        For A DataFrame with line, this function will extract both the entities.\n",
        "\n",
        "        These values will be added as Two Columns Named 'e1' and 'e2' in the DataFrame and will be returned.\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      dataframe['entities'] = dataframe['line']\n",
        "      entity_dict = {}\n",
        "      entity_type = {}\n",
        "\n",
        "      for i, val in enumerate(dataframe['entities']):\n",
        "        e1 = re.findall('<e1>(.*?)</e1>', val)\n",
        "        e2 = re.findall('<e2>(.*?)</e2>', val)\n",
        "        entity_dict[i+1] = (str(e1[0]), str(e2[0]))\n",
        "        doc = nlp(e1[0])\n",
        "        for ent in doc.ents:\n",
        "          if ent.label_:\n",
        "            entity_type[i] = (ent.label_, 'NONE')\n",
        "        \n",
        "        doc = nlp(e2[0])\n",
        "        for ent in doc.ents:\n",
        "          if ent.label_:\n",
        "            entity_type[i] = (entity_type[i][0], ent.label_)\n",
        "\n",
        "      entity_dataframe = self.create_dataframe(entity_dict, ['e1', 'e2'])\n",
        "      entity_type_df   = self.create_dataframe(entity_type, ['e1', 'e2'])\n",
        "\n",
        "      dataframe = dataframe.drop(columns=['entities'])\n",
        "      dataframe['e1'] = entity_dataframe['e1']\n",
        "      dataframe['e2'] = entity_dataframe['e2']\n",
        "      dataframe['e1_type'] = entity_type_df['e1']\n",
        "      dataframe['e2_type'] = entity_type_df['e2']\n",
        "\n",
        "      return dataframe\n",
        "\n",
        "    def print_all_hyps(self, dataframe, col):\n",
        "\n",
        "      for i, val in enumerate(ans_df[col]):\n",
        "        val = val.replace(' ', '')\n",
        "        string = val.split(',')\n",
        "        print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "        print(\"Sentence : \", i+1, \"\\n\")\n",
        "        print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "        for word in string:\n",
        "          if wn.synsets(word):\n",
        "\n",
        "            syn = wn.synsets(word)[0]\n",
        "            print(\"\\n\")\n",
        "            print(\"Word: \",word)\n",
        "            print(\"Holonyms   :\", wn.synsets(word)[0].part_holonyms())\n",
        "            print(\"Meronyms   :\", wn.synsets(word)[0].part_meronyms())\n",
        "            print(\"HyperNyms  :\", syn.hypernyms())\n",
        "            print(\"HypoNyms   :\", syn.hyponyms())\n",
        "\n",
        "\n",
        "\n",
        "    def create_hyper(self, dataframe, col):\n",
        "\n",
        "      hypernym = {}\n",
        "      hyper = []\n",
        "      all_hyper = []\n",
        "\n",
        "      for i, val in enumerate(ans_df[col]):\n",
        "        val = val.replace(' ', '')\n",
        "        string = val.split(',')\n",
        "        for word in string:\n",
        "          if wn.synsets(word):\n",
        "\n",
        "            syn = wn.synsets(word)[0]\n",
        "            hype = syn.hypernyms()\n",
        "\n",
        "            if hype:\n",
        "\n",
        "              for value in hype:\n",
        "                hyper.append(str(value)[8:-3].split('.')[0])\n",
        "\n",
        "              all_hyper.append(word + ' : ' + ', '.join(v for v in hyper))\n",
        "              hyper = []\n",
        "          \n",
        "        hypernym[i] = ', '.join(v for v in all_hyper)\n",
        "        all_hyper = []\n",
        "      colname = 'hyp'\n",
        "      hypernym_dataframe = self.create_dataframe(hypernym, [colname])\n",
        "      dataframe[colname] = hypernym_dataframe[colname]\n",
        "\n",
        "      return dataframe\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "    def create_holo(self, dataframe, col):\n",
        "\n",
        "      holonym = {}\n",
        "      holo = []\n",
        "      all_holo = []\n",
        "\n",
        "      for i, val in enumerate(ans_df[col]):\n",
        "        val = val.replace(' ', '')\n",
        "        string = val.split(',')\n",
        "        for word in string:\n",
        "          if wn.synsets(word):\n",
        "\n",
        "            hol = wn.synsets(word)[0].part_holonyms()\n",
        "            if hol:\n",
        "              for value in hol:\n",
        "                holo.append(str(value)[8:-3].split('.')[0])\n",
        "\n",
        "              all_holo.append(word + ' : ' + ', '.join(v for v in holo))\n",
        "              hol = []\n",
        "          \n",
        "        holonym[i] = ', '.join(v for v in all_holo)\n",
        "        all_holo = []\n",
        "      colname = 'holo'\n",
        "      holonym_dataframe = self.create_dataframe(holonym, [colname])\n",
        "      dataframe[colname] = holonym_dataframe[colname]\n",
        "\n",
        "      return dataframe\n",
        "\n",
        "    def create_mero(self, dataframe, col):\n",
        "\n",
        "      meronym = {}\n",
        "      mero = []\n",
        "      all_mero = []\n",
        "\n",
        "      for i, val in enumerate(ans_df[col]):\n",
        "        val = val.replace(' ', '')\n",
        "        string = val.split(',')\n",
        "        for word in string:\n",
        "          if wn.synsets(word):\n",
        "            mer = wn.synsets(word)[0].part_meronyms()\n",
        "          \n",
        "            if mer:\n",
        "              for value in mer:\n",
        "                mero.append(str(value)[8:-3].split('.')[0])\n",
        "\n",
        "              all_mero.append(word + ' : ' + ', '.join(v for v in mero))\n",
        "              mer = []\n",
        "        meronym[i] = ', '.join(v for v in all_mero)\n",
        "        all_mero = []\n",
        "      colname = 'mero'\n",
        "      meronym_dataframe = self.create_dataframe(meronym, [colname])\n",
        "      dataframe[colname] = meronym_dataframe[colname]\n",
        "\n",
        "      return dataframe\n",
        "\n",
        "\n",
        "    def create_hypo(self, dataframe, col):\n",
        "\n",
        "      hyponym = {}\n",
        "      hypo = []\n",
        "      all_hypo = []\n",
        "\n",
        "      for i, val in enumerate(ans_df[col]):\n",
        "        val = val.replace(' ', '')\n",
        "        string = val.split(',')\n",
        "        for word in string:\n",
        "          if wn.synsets(word):\n",
        "\n",
        "            syn = wn.synsets(word)[0]\n",
        "            hyp = syn.hyponyms()\n",
        "\n",
        "            if hyp:\n",
        "              for value in hyp:\n",
        "                hypo.append(str(value)[8:-3].split('.')[0])\n",
        "\n",
        "              all_hypo.append(word + ' : ' + ', '.join(v for v in hypo))\n",
        "              hyp = []\n",
        "        hyponym[i] = ', '.join(v for v in all_hypo)\n",
        "        all_hypo = []\n",
        "      colname = 'hypo'\n",
        "      hyponym_dataframe = self.create_dataframe(hyponym, [colname])\n",
        "      dataframe[colname] = hyponym_dataframe[colname]\n",
        "\n",
        "      return dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xtEF3zJsLBC",
        "outputId": "c1cf4aa5-c164-4d8e-a5e3-1e1028f387bc"
      },
      "source": [
        "task2 = AllTasks()\n",
        "ans_df = task2.create_tokens(ans_df)\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"The Tokens are          : \", ans_df['tokens'][index])\n",
        "  print(\"The Filtered Tokens are : \", ans_df['filtered tokens'][index])\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Tokens are          :  After, handsome, renovations, at, various, locales, ,, the, <e1>, company, </e1>, has, remodeled, a, church, into, a, <e2>, home, </e2>, .\n",
            "The Filtered Tokens are :  After, handsome, renovations, at, various, locales, ,, the, company, has, remodeled, a, church, into, a, home, .\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Tokens are          :  The, <e1>, laptop, </e1>, was, inside, a, protective, <e2>, plastic, bag, </e2>, made, from, LDPE, number, 4, ,, which, can, be, easily, recycled, .\n",
            "The Filtered Tokens are :  The, laptop, was, inside, a, protective, plastic, bag, made, from, LDPE, number, 4, ,, which, can, be, easily, recycled, .\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZAAJZbdsXie",
        "outputId": "33fc87cc-379f-41f9-fae0-230b64a66c1a"
      },
      "source": [
        "ans_df = task2.create_pos_dep_lemma(ans_df, 'filtered tokens')\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"The Filtered Tokens are     : \", ans_df['filtered tokens'][index], \"\\n\")\n",
        "  print(\"The Lemmas are              : \", ans_df['lem'][index], \"\\n\")\n",
        "  print(\"The POS Tags are            : \", ans_df['pos'][index], \"\\n\")\n",
        "  print(\"The Dependency Parse is     : \", ans_df['dep'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Filtered Tokens are     :  After, handsome, renovations, at, various, locales, ,, the, company, has, remodeled, a, church, into, a, home, . \n",
            "\n",
            "The Lemmas are              :  after, handsome, renovation, at, various, locale,  , the, company, have, remodel, a, church, into, a, home, . \n",
            "\n",
            "The POS Tags are            :  ADP, ADJ, NOUN, ADP, ADJ, NOUN, SPACE, DET, NOUN, AUX, VERB, DET, NOUN, ADP, DET, NOUN, PUNCT \n",
            "\n",
            "The Dependency Parse is     :  prep, amod, pobj, prep, amod, pobj, , det, nsubj, aux, ROOT, det, dobj, prep, det, pobj, punct \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The Filtered Tokens are     :  The, laptop, was, inside, a, protective, plastic, bag, made, from, LDPE, number, 4, ,, which, can, be, easily, recycled, . \n",
            "\n",
            "The Lemmas are              :  the, laptop, be, inside, a, protective, plastic, bag, make, from, ldpe, number, 4,  , which, can, be, easily, recycle, . \n",
            "\n",
            "The POS Tags are            :  DET, NOUN, AUX, ADP, DET, ADJ, ADJ, NOUN, VERB, ADP, NOUN, NOUN, NUM, SPACE, DET, VERB, AUX, ADV, VERB, PUNCT \n",
            "\n",
            "The Dependency Parse is     :  det, nsubj, ROOT, prep, det, amod, amod, pobj, acl, prep, compound, pobj, nummod, , nsubjpass, aux, auxpass, advmod, relcl, punct \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "Jch7NdgStOjR",
        "outputId": "c0491675-4211-4891-9ac9-9b2a0a0e0872"
      },
      "source": [
        "ans_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line</th>\n",
              "      <th>relation</th>\n",
              "      <th>tokens</th>\n",
              "      <th>filtered tokens</th>\n",
              "      <th>pos</th>\n",
              "      <th>dep</th>\n",
              "      <th>lem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>After handsome renovations at various locales,...</td>\n",
              "      <td>Other</td>\n",
              "      <td>After, handsome, renovations, at, various, loc...</td>\n",
              "      <td>After, handsome, renovations, at, various, loc...</td>\n",
              "      <td>ADP, ADJ, NOUN, ADP, ADJ, NOUN, SPACE, DET, NO...</td>\n",
              "      <td>prep, amod, pobj, prep, amod, pobj, , det, nsu...</td>\n",
              "      <td>after, handsome, renovation, at, various, loca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The &lt;e1&gt;laptop&lt;/e1&gt; was inside a protective &lt;e...</td>\n",
              "      <td>Content-Container(e1,e2)</td>\n",
              "      <td>The, &lt;e1&gt;, laptop, &lt;/e1&gt;, was, inside, a, prot...</td>\n",
              "      <td>The, laptop, was, inside, a, protective, plast...</td>\n",
              "      <td>DET, NOUN, AUX, ADP, DET, ADJ, ADJ, NOUN, VERB...</td>\n",
              "      <td>det, nsubj, ROOT, prep, det, amod, amod, pobj,...</td>\n",
              "      <td>the, laptop, be, inside, a, protective, plasti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                line  ...                                                lem\n",
              "0  After handsome renovations at various locales,...  ...  after, handsome, renovation, at, various, loca...\n",
              "1  The <e1>laptop</e1> was inside a protective <e...  ...  the, laptop, be, inside, a, protective, plasti...\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlL8mpVh0MAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b46ae0-3cf1-4d48-9b23-979d6696d44f"
      },
      "source": [
        "ans_df = task2.create_NER(ans_df)\n",
        "\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"Sentence :\", index+1, \"\\n\")\n",
        "  print(\"The Entity 1 is              : \", ans_df['e1'][index], \"\\n\")\n",
        "  print(\"The Entity 2 is              : \", ans_df['e2'][index], \"\\n\")\n",
        "  print(\"The Type of Entity 1 is      : \", ans_df['e1_type'][index], \"\\n\")\n",
        "  print(\"The Type of Entity 2 is      : \", ans_df['e2_type'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 1 \n",
            "\n",
            "The Entity 1 is              :  company \n",
            "\n",
            "The Entity 2 is              :  home \n",
            "\n",
            "The Type of Entity 1 is      :  nan \n",
            "\n",
            "The Type of Entity 2 is      :  nan \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 2 \n",
            "\n",
            "The Entity 1 is              :  laptop \n",
            "\n",
            "The Entity 2 is              :  plastic bag \n",
            "\n",
            "The Type of Entity 1 is      :  nan \n",
            "\n",
            "The Type of Entity 2 is      :  nan \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIQlZr4WlM46",
        "outputId": "b0b36d41-2a88-4175-a20d-4d567b20b09b"
      },
      "source": [
        "task2.print_all_hyps(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence :  1 \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Word:  After\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  handsome\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  renovations\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('improvement.n.02')]\n",
            "HypoNyms   : [Synset('face_lift.n.02')]\n",
            "\n",
            "\n",
            "Word:  at\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('chemical_element.n.01'), Synset('halogen.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  various\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  locales\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('scene.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  company\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('institution.n.01')]\n",
            "HypoNyms   : [Synset('broadcasting_company.n.01'), Synset('bureau_de_change.n.01'), Synset('car_company.n.01'), Synset('closed_shop.n.01'), Synset('corporate_investor.n.01'), Synset('distributor.n.03'), Synset('dot-com.n.01'), Synset('drug_company.n.01'), Synset('east_india_company.n.01'), Synset('electronics_company.n.01'), Synset('film_company.n.01'), Synset('food_company.n.01'), Synset('furniture_company.n.01'), Synset('holding_company.n.01'), Synset('joint-stock_company.n.01'), Synset('limited_company.n.01'), Synset('livery_company.n.01'), Synset('mining_company.n.01'), Synset('mover.n.04'), Synset('oil_company.n.01'), Synset('open_shop.n.01'), Synset('packaging_company.n.01'), Synset('pipeline_company.n.01'), Synset('printing_concern.n.01'), Synset('record_company.n.01'), Synset('service.n.04'), Synset('shipper.n.02'), Synset('shipping_company.n.01'), Synset('steel_company.n.01'), Synset('stock_company.n.01'), Synset('subsidiary_company.n.01'), Synset('target_company.n.01'), Synset('think_tank.n.01'), Synset('transportation_company.n.01'), Synset('union_shop.n.01'), Synset('white_knight.n.01')]\n",
            "\n",
            "\n",
            "Word:  has\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('angular_distance.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  remodeled\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('change.v.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  a\n",
            "Holonyms   : [Synset('nanometer.n.01')]\n",
            "Meronyms   : [Synset('picometer.n.01')]\n",
            "HyperNyms  : [Synset('metric_linear_unit.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  church\n",
            "Holonyms   : [Synset('christendom.n.01')]\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('religion.n.02')]\n",
            "HypoNyms   : [Synset('armenian_church.n.01'), Synset('catholic_church.n.01'), Synset('coptic_church.n.01'), Synset('nestorian_church.n.01'), Synset('protestant_church.n.01'), Synset('unification_church.n.01')]\n",
            "\n",
            "\n",
            "Word:  a\n",
            "Holonyms   : [Synset('nanometer.n.01')]\n",
            "Meronyms   : [Synset('picometer.n.01')]\n",
            "HyperNyms  : [Synset('metric_linear_unit.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  home\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('residence.n.01')]\n",
            "HypoNyms   : [Synset('home_away_from_home.n.01')]\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence :  2 \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Word:  laptop\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('portable_computer.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  was\n",
            "Holonyms   : [Synset('united_states.n.01')]\n",
            "Meronyms   : [Synset('aberdeen.n.01'), Synset('adams.n.04'), Synset('bellingham.n.01'), Synset('cape_flattery.n.01'), Synset('columbia.n.01'), Synset('inland_passage.n.01'), Synset('kennewick.n.01'), Synset('lake_chelan.n.01'), Synset('mount_ranier_national_park.n.01'), Synset('mount_saint_helens.n.01'), Synset('north_cascades_national_park.n.01'), Synset('olympia.n.01'), Synset('olympic_national_park.n.01'), Synset('pacific_northwest.n.01'), Synset('puget_sound.n.01'), Synset('ranier.n.01'), Synset('scablands.n.01'), Synset('seattle.n.01'), Synset('snake.n.03'), Synset('spokane.n.01'), Synset('tacoma.n.01'), Synset('vancouver.n.02'), Synset('walla_walla.n.01'), Synset('yakima.n.01')]\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  inside\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('region.n.01')]\n",
            "HypoNyms   : [Synset('midland.n.02'), Synset('midst.n.01'), Synset('penetralia.n.01')]\n",
            "\n",
            "\n",
            "Word:  a\n",
            "Holonyms   : [Synset('nanometer.n.01')]\n",
            "Meronyms   : [Synset('picometer.n.01')]\n",
            "HyperNyms  : [Synset('metric_linear_unit.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  protective\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  plastic\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('solid.n.01')]\n",
            "HypoNyms   : [Synset('acrylonitrile-butadiene-styrene.n.01'), Synset('amino_plastic.n.01'), Synset('bakelite.n.01'), Synset('cellulosic.n.01'), Synset('coumarone-indene_resin.n.01'), Synset('fluorocarbon_plastic.n.01'), Synset('mylar.n.01'), Synset('phenolic_plastic.n.01'), Synset('polyester.n.02'), Synset('polypropylene.n.01'), Synset('polyvinyl-formaldehyde.n.01'), Synset('resinoid.n.01'), Synset('silicone_resin.n.01'), Synset('teflon.n.01'), Synset('thermoplastic.n.01'), Synset('thermosetting_compositions.n.01'), Synset('vinyl.n.02'), Synset('vinylite.n.01')]\n",
            "\n",
            "\n",
            "Word:  bag\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('container.n.01')]\n",
            "HypoNyms   : [Synset('air_bag.n.01'), Synset('backpack.n.01'), Synset('beanbag.n.01'), Synset('bladder.n.02'), Synset('body_bag.n.01'), Synset('book_bag.n.01'), Synset('burn_bag.n.01'), Synset('carryall.n.01'), Synset('drawstring_bag.n.01'), Synset('dust_bag.n.01'), Synset('envelope.n.06'), Synset('gamebag.n.01'), Synset('golf_bag.n.01'), Synset('gunnysack.n.01'), Synset('ice_pack.n.01'), Synset('mailbag.n.02'), Synset('nosebag.n.01'), Synset('pannier.n.01'), Synset('plastic_bag.n.01'), Synset('pouch.n.01'), Synset('purse.n.03'), Synset('ragbag.n.02'), Synset('rosin_bag.n.01'), Synset('sachet.n.01'), Synset('sack.n.01'), Synset('saddlebag.n.01'), Synset('sandbag.n.01'), Synset('schoolbag.n.01'), Synset('shopping_bag.n.01'), Synset('sick_bag.n.01'), Synset('skin.n.06'), Synset('sleeping_bag.n.01'), Synset('sweat_bag.n.01'), Synset('tea_bag.n.02'), Synset('toilet_bag.n.01'), Synset('tool_bag.n.01'), Synset('tucker-bag.n.01')]\n",
            "\n",
            "\n",
            "Word:  made\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : [Synset('overdo.v.01')]\n",
            "\n",
            "\n",
            "Word:  number\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('amount.n.02')]\n",
            "HypoNyms   : [Synset('fewness.n.01'), Synset('innumerableness.n.01'), Synset('majority.n.01'), Synset('minority.n.02'), Synset('numerousness.n.01'), Synset('preponderance.n.02'), Synset('roundness.n.02')]\n",
            "\n",
            "\n",
            "Word:  4\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('digit.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  can\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('container.n.01')]\n",
            "HypoNyms   : [Synset('beer_can.n.01'), Synset('caddy.n.01'), Synset('cannikin.n.02'), Synset('coffee_can.n.01'), Synset('milk_can.n.01'), Synset('oilcan.n.01'), Synset('soda_can.n.01')]\n",
            "\n",
            "\n",
            "Word:  be\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('metallic_element.n.01')]\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  easily\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : []\n",
            "HypoNyms   : []\n",
            "\n",
            "\n",
            "Word:  recycled\n",
            "Holonyms   : []\n",
            "Meronyms   : []\n",
            "HyperNyms  : [Synset('cycle.v.01')]\n",
            "HypoNyms   : []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmGI6tbqLrFb"
      },
      "source": [
        "ans_df = task2.create_hyper(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zO2vbnyYu8_"
      },
      "source": [
        "ans_df = task2.create_mero(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDKl5uFYu3-"
      },
      "source": [
        "ans_df = task2.create_holo(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36yBdgihSYVt"
      },
      "source": [
        "ans_df = task2.create_hypo(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdL_8aoBVAqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "e9714f76-5bca-4574-cddc-4ce3542d491a"
      },
      "source": [
        "ans_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>line</th>\n",
              "      <th>relation</th>\n",
              "      <th>tokens</th>\n",
              "      <th>filtered tokens</th>\n",
              "      <th>pos</th>\n",
              "      <th>dep</th>\n",
              "      <th>lem</th>\n",
              "      <th>e1</th>\n",
              "      <th>e2</th>\n",
              "      <th>e1_type</th>\n",
              "      <th>e2_type</th>\n",
              "      <th>hyp</th>\n",
              "      <th>mero</th>\n",
              "      <th>holo</th>\n",
              "      <th>hypo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>After handsome renovations at various locales,...</td>\n",
              "      <td>Other</td>\n",
              "      <td>After, handsome, renovations, at, various, loc...</td>\n",
              "      <td>After, handsome, renovations, at, various, loc...</td>\n",
              "      <td>ADP, ADJ, NOUN, ADP, ADJ, NOUN, SPACE, DET, NO...</td>\n",
              "      <td>prep, amod, pobj, prep, amod, pobj, , det, nsu...</td>\n",
              "      <td>after, handsome, renovation, at, various, loca...</td>\n",
              "      <td>company</td>\n",
              "      <td>home</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>renovations : improvement, at : chemical_eleme...</td>\n",
              "      <td>a : picometer, a : picometer, picometer</td>\n",
              "      <td>a : nanometer, church : nanometer, christendom...</td>\n",
              "      <td>renovations : face_lift, company : face_lift, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The &lt;e1&gt;laptop&lt;/e1&gt; was inside a protective &lt;e...</td>\n",
              "      <td>Content-Container(e1,e2)</td>\n",
              "      <td>The, &lt;e1&gt;, laptop, &lt;/e1&gt;, was, inside, a, prot...</td>\n",
              "      <td>The, laptop, was, inside, a, protective, plast...</td>\n",
              "      <td>DET, NOUN, AUX, ADP, DET, ADJ, ADJ, NOUN, VERB...</td>\n",
              "      <td>det, nsubj, ROOT, prep, det, amod, amod, pobj,...</td>\n",
              "      <td>the, laptop, be, inside, a, protective, plasti...</td>\n",
              "      <td>laptop</td>\n",
              "      <td>plastic bag</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>laptop : portable_computer, inside : region, a...</td>\n",
              "      <td>was : picometer, picometer, aberdeen, adams, b...</td>\n",
              "      <td>was : nanometer, christendom, nanometer, unite...</td>\n",
              "      <td>inside : face_lift, broadcasting_company, bure...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                line  ...                                               hypo\n",
              "0  After handsome renovations at various locales,...  ...  renovations : face_lift, company : face_lift, ...\n",
              "1  The <e1>laptop</e1> was inside a protective <e...  ...  inside : face_lift, broadcasting_company, bure...\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_H3x6UYl9ux",
        "outputId": "5631c55f-1fd1-47e2-e32b-0267f96d05b1"
      },
      "source": [
        "task2.synctactic_parse(ans_df, 'filtered tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence :  1 \n",
            "\n",
            "\n",
            "Text\t\t\t\t : Root Text\t\t\t : Root Dep\t\t\t : Root Head\t\t\t : Root Head Pos\t|\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "handsome renovations \t\t : renovations \t\t\t : pobj \t\t\t : After \t\t\t : ADP \t |\n",
            "\n",
            "various locales \t\t : locales \t\t\t : pobj \t\t\t : at \t\t\t : ADP \t |\n",
            "\n",
            "the company \t\t : company \t\t\t : nsubj \t\t\t : remodeled \t\t\t : VERB \t |\n",
            "\n",
            "a church \t\t : church \t\t\t : dobj \t\t\t : remodeled \t\t\t : VERB \t |\n",
            "\n",
            "a home \t\t : home \t\t\t : pobj \t\t\t : into \t\t\t : ADP \t |\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence :  2 \n",
            "\n",
            "\n",
            "Text\t\t\t\t : Root Text\t\t\t : Root Dep\t\t\t : Root Head\t\t\t : Root Head Pos\t|\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "The laptop \t\t : laptop \t\t\t : nsubj \t\t\t : was \t\t\t : AUX \t |\n",
            "\n",
            "a protective plastic bag \t\t : bag \t\t\t : pobj \t\t\t : inside \t\t\t : ADP \t |\n",
            "\n",
            "LDPE number \t\t : number \t\t\t : pobj \t\t\t : from \t\t\t : ADP \t |\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9aqJ-20ntFw"
      },
      "source": [
        "# Task 2: Extra Features Extracted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeKQZX6QnUBl"
      },
      "source": [
        "class Task2Extra(AllTasks):\n",
        "\n",
        "  def create_window(self, dataframe):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "      For A DataFrame with the column 'line', this function will create a Window\n",
        "      of the words from E1 Tag - 1 to E2 Tag + 1 words.\n",
        "\n",
        "      This window will be added as a New Column Named 'Line Window' in the DataFrame and will be returned\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    window_dict = {}\n",
        "    for i, val in enumerate(dataframe['line']):\n",
        "      e1 = re.findall('<e1>(.*?)</e2>', val)\n",
        "      before = re.findall('\\w* ?<e1>', val)\n",
        "      after = re.findall('</e2> ?\\w*', val)\n",
        "      bef = before[0].replace('<e1>', '')\n",
        "      aft = after[0].replace('</e2>', '')\n",
        "      s = e1[0].replace('</e1>', '').replace('<e2>', '')\n",
        "      window_dict[i] = bef+s+aft\n",
        "\n",
        "\n",
        "    window_dataframe = self.create_dataframe(window_dict, ['window'])\n",
        "\n",
        "    dataframe['line window'] = window_dataframe['window']\n",
        "    return dataframe\n",
        "\n",
        "  def create_syn(self, dataframe, col):\n",
        "    \"\"\"\n",
        "\n",
        "      For A DataFrame with col, this function will extract synsets of both the entities.\n",
        "\n",
        "      These values will be added as Two Columns Named 'e1_syn' and 'e2_syn' in the DataFrame and will be returned.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    hypernym_e = {}\n",
        "    hyper = []\n",
        "    for i, val in enumerate(dataframe[col]):\n",
        "      if wn.synsets(val):\n",
        "        syn = wn.synsets(val)[0]\n",
        "        s = syn.hypernyms()\n",
        "        if s:\n",
        "          for val in s:\n",
        "            hyper.append(str(val)[8:-3].split('.')[0])\n",
        "\n",
        "          hypernym_e[i] = ', '.join(v for v in hyper)\n",
        "          hyper = []\n",
        "        else:\n",
        "          hypernym_e[i] = 'None'\n",
        "      else:\n",
        "        hypernym_e[i] = 'None'\n",
        "\n",
        "      \n",
        "    colname = col + '_syn'\n",
        "\n",
        "    hypernym_e_dataframe = self.create_dataframe(hypernym_e, [colname])\n",
        "    dataframe[colname] = hypernym_e_dataframe[colname]\n",
        "\n",
        "    return dataframe\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbE7BwjWDPJq"
      },
      "source": [
        "task2extra = Task2Extra()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNENtkwzCmT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f26c151-74c3-42b1-dcfa-7b604c6fd605"
      },
      "source": [
        "ans_df['just_relation'] = [x.split('(')[0] for x in ans_df['relation']]\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"Sentence :\", index+1, \"\\n\")\n",
        "  print(\"The Relation with Direction is      : \", ans_df['relation'][index], \"\\n\")\n",
        "  print(\"The Relation without Direction is   : \", ans_df['just_relation'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 1 \n",
            "\n",
            "The Relation with Direction is      :  Other \n",
            "\n",
            "The Relation without Direction is   :  Other \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 2 \n",
            "\n",
            "The Relation with Direction is      :  Content-Container(e1,e2) \n",
            "\n",
            "The Relation without Direction is   :  Content-Container \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t5FcHtQDMWT",
        "outputId": "6ec8e137-649c-4efc-90fc-3d31a3247a0f"
      },
      "source": [
        "ans_df = task2extra.create_window(ans_df)\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"Sentence :\", index+1, \"\\n\")\n",
        "  print(\"The Full Line is           : \", ans_df['line'][index], \"\\n\")\n",
        "  print(\"The Created Window is      : \", ans_df['line window'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 1 \n",
            "\n",
            "The Full Line is           :  After handsome renovations at various locales, the <e1>company</e1> has remodeled a church into a <e2>home</e2>.\n",
            " \n",
            "\n",
            "The Created Window is      :  the company has remodeled a church into a home \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 2 \n",
            "\n",
            "The Full Line is           :  The <e1>laptop</e1> was inside a protective <e2>plastic bag</e2> made from LDPE number 4, which can be easily recycled.\n",
            " \n",
            "\n",
            "The Created Window is      :  The laptop was inside a protective plastic bag made \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu_Iv3sYDY_m",
        "outputId": "d76e4c59-f545-4bc2-ec97-2e186b17b9f9"
      },
      "source": [
        "ans_df = task2extra.create_syn(ans_df, 'e1')\n",
        "ans_df = task2extra.create_syn(ans_df, 'e2')\n",
        "\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"Sentence :\", index+1, \"\\n\")\n",
        "  print(\"The Entity 1 is           : \", ans_df['e1'][index], \"\\n\")\n",
        "  print(\"The Synset for E1 are     : \", ans_df['e1_syn'][index], \"\\n\")\n",
        "  print(\"The Entity 2 is           : \", ans_df['e2'][index], \"\\n\")\n",
        "  print(\"The Synset for E2 are     : \", ans_df['e2_syn'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 1 \n",
            "\n",
            "The Entity 1 is           :  company \n",
            "\n",
            "The Synset for E1 are     :  institution \n",
            "\n",
            "The Entity 2 is           :  home \n",
            "\n",
            "The Synset for E2 are     :  residence \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 2 \n",
            "\n",
            "The Entity 1 is           :  laptop \n",
            "\n",
            "The Synset for E1 are     :  portable_computer \n",
            "\n",
            "The Entity 2 is           :  plastic bag \n",
            "\n",
            "The Synset for E2 are     :  None \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs3Gl6biD5P_",
        "outputId": "8a4c4765-02c7-4327-f74e-27bcd4c427f1"
      },
      "source": [
        "ans_df = task2extra.create_pos_dep_lemma(ans_df, 'line window')\n",
        "\n",
        "\n",
        "for index in ans_df.index:\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")\n",
        "  print(\"Sentence :\", index+1, \"\\n\")\n",
        "  print(\"The Line is                      : \", ans_df['line'][index], \"\\n\")\n",
        "  print(\"The Line Window                  : \", ans_df['line window'][index], \"\\n\")\n",
        "  print(\"The Shortest Depedency Path is   : \", ans_df['line window_dep'][index], \"\\n\")\n",
        "  print(\"The POS Tags for the Window is   : \", ans_df['line window_pos'][index], \"\\n\")\n",
        "  print(\"\\n----------------------------------------------------------------------------------------------------------------------------------------\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 1 \n",
            "\n",
            "The Line is                      :  After handsome renovations at various locales, the <e1>company</e1> has remodeled a church into a <e2>home</e2>.\n",
            " \n",
            "\n",
            "The Line Window                  :  the company has remodeled a church into a home \n",
            "\n",
            "The Shortest Depedency Path is   :  det, nsubj, aux, ROOT, det, dobj, prep, det, pobj \n",
            "\n",
            "The POS Tags for the Window is   :  DET, NOUN, AUX, VERB, DET, NOUN, ADP, DET, NOUN \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence : 2 \n",
            "\n",
            "The Line is                      :  The <e1>laptop</e1> was inside a protective <e2>plastic bag</e2> made from LDPE number 4, which can be easily recycled.\n",
            " \n",
            "\n",
            "The Line Window                  :  The laptop was inside a protective plastic bag made \n",
            "\n",
            "The Shortest Depedency Path is   :  det, nsubj, ROOT, prep, det, amod, amod, pobj, acl \n",
            "\n",
            "The POS Tags for the Window is   :  DET, NOUN, AUX, ADP, DET, ADJ, ADJ, NOUN, VERB \n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7DhKICAvoqA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}